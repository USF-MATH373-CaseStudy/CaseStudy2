---
title: "Case Study 2"
author: "Sahil Jain"
date: "April 6, 2018"
output: html_document
---

Loading Libraries 

```{r}
library(tree)
library(ISLR)
library(randomForest)
library(MASS)
library(caret)
library(TSA)
library(ipred)
library(rpart)
library(gbm)
library(TH.data)
```

Loading training data 

```{r}
train <- read.csv("/Users/sahiljain/Desktop/Spring 2018/Statistical Learning/OnlineNewsPopularityTraining.csv")
test <- read.csv("/Users/sahiljain/Desktop/Spring 2018/Statistical Learning/OnlineNewsPopularityTest.csv")
```

Omitting NA's 

```{r}
dataTrain <- na.omit(train)
dataTest <- na.omit(test)
```

Omitting extra variables 

```{r}
NewData_train <- dataTrain[,-c(1,2,61)]
NewData_test <- dataTest[,-c(1,2,61)]
```

```{r}
popularity <- data.frame(NewData_train, NewData_train$popular)
popularity_test <- data.frame(NewData_test)
```

(A) Build classifiers for this data set using the tree based methods that weâ€™ve learned in class. In particular, build classifiers on the training data and assess the performance of each of the following methods on the test data (available on Canvas)

(I) Classification tree

```{r}
tree.popularity <- tree(NewData_train$popular ~ ., NewData_train)
summary(tree.popularity)
```

```{r}
plot(tree.popularity)
text(tree.popularity, pretty = 0)
```

```{r}
tree.popularity
```

Looking at the prediction on the test set 

```{r}
tree.pred <- predict(tree.popularity, popularity)
```

Evaluating MisClassifications 

```{r}
table(tree.pred, popularity$popular)
```

Calculating Accuracy 

```{r}
mean(tree.pred == popularity$popular)
```

hmmm...  strange accuracy is zero... 

Cross Validation for Classification tree 

```{r}
set.seed(3)

cv.popularity <- cv.tree(tree.popularity, FUN = prune.misclass)

cv.popularity
```

(II) Bagging

```{r}
gbag <- bagging(popular ~., data = NewData_train, coob = TRUE)
print(gbag)
```

Prediction on training set and measuring MSPE 

```{r}
yhat <- predict(gbag, newdata = NewData_train)
sqrt(mean((yhat - NewData_train$popular)^2))
```

Here, from our training set out MSPE is 0.3931 which is quite small, now we will see our MSPE and prediction on the held out test set. 

Prediction and MSPE on held out test set 

```{r}
gbag_test <- bagging(popular ~., data = NewData_test, coob = TRUE)
print(gbag_test)
```

```{r}
yhat_test <- predict(gbag_test, newdata = NewData_test)
sqrt(mean((yhat_test - NewData_test$popular)^2))
```
We can see that out MSPE from our held out test set is 0.3936, whihc is slightly higher than our MSPE on training set.

(III) Random Forest

We will be running our Random Forest over 1000 trees and all 58 predictors. 

```{r}
set.seed(1)
bag.popular <- randomForest(popular ~., data = NewData_train, mtry = 58, importance = TRUE, ntrees = 1000)
bag.popular
```

Prediction on Training set and measuring MSPE

```{r}
yhat.bag <- predict(bag.popular, newdata = NewData_train)
sqrt(mean((yhat.bag - NewData_train$popular)^2))
```

As we can see our MSPE is incredibly small close to 0.16 predictors. Now we will investigate the performance on a held out test set. 

```{r}
set.seed(1)
rf.popular <- randomForest(popular ~ ., data = NewData_test, mtry = 58, importance = TRUE)
```

```{r}
yhat.rf <- predict(rf.popular, newdata = NewData_test)
sqrt(mean((yhat.rf - NewData_test$popular)^2))
```

From our held out test set our MSPE turns out to be 0.1625 which in slightly less than our training set

We will now see the imprtance of each variable in the random forest. 

Calculating the importance of each predictor 
```{r}
importance(rf.popular)
```

Too much information, now we will plot the importance of each variable. 

Plotting the importance measures of each variable.

```{r}
varImpPlot(rf.popular)
```

From the above result we can clearly see that variable kw_avg_avg is by far the most important variable in the random forest.


(IV) Boosting - Boosting is very similar to randomForests

```{r}
boost <- gbm(popular ~., data = NewData_train, distribution = 'gaussian', n.trees = 5000, interaction.depth = 4)
summary(boost)
```

Clearly, we can see that kw_avg_avg is by far the most important variable. Now we can look at the plot of this variable.

```{r}
plot(boost, i = 'kw_avg_avg')
```

Now we will predict our training set and measure training MSPE

```{r}
boost.train <- predict(boost, newdata = NewData_train, n.trees = 5000)
sqrt(mean((boost.train - NewData_train$popular)^2))
```

Using boosting we can clearly see that our MSPE is incredibly small and close to 0.37 predictors. Now we will investigate the performance on a held out test set. 

Appling our boosted model to predit the test set 

```{r}
boost.pred <- predict(boost, NewData_test, n.trees = 5000)
sqrt(mean((boost.pred - NewData_test$popular)^2))
```

From our held out test set our MSPE turns out to be 0.393 which in slightly higher than our training set


OVERVIEW: 

